{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Requirements"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756917479c4995ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Install necessary Python packages:\n",
    "   - pandas\n",
    "   - matplotlib\n",
    "   - numpy\n",
    "   - opencv-python (cv2)\n",
    "   - tqdm\n",
    "   - detectron2 (for object detection)\n",
    "   - torch (for deep learning models)\n",
    "   - torchvision (for data transformations)\n",
    "   - efficientnet_pytorch (for feature extraction using EfficientNet)\n",
    "   - scikit-learn (for data splitting and evaluation)\n",
    "   \n",
    "2. Command to install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib numpy opencv-python tqdm detectron2 torch torchvision efficientnet_pytorch scikit-learn\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629be34eed45b73a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff8427dba110edd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "dataset/\n",
    "├── arm_flapping/\n",
    "│   ├── video1.mp4\n",
    "│   ├── video2.mp4\n",
    "│   └── ...\n",
    "├── headbanging/\n",
    "│   ├── video1.mp4\n",
    "│   ├── video2.mp4\n",
    "│   └── ...\n",
    "└── spinning/\n",
    "    ├── video1.mp4\n",
    "    ├── video2.mp4\n",
    "    └── ...\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e33c3f1f0d6a7ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_csv(dataset_dir, output_csv):\n",
    "    classes = ['arm_flapping', 'headbanging', 'spinning', 'handaction']\n",
    "    data = []\n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.mp4'):\n",
    "                filepath = os.path.join(class_dir, filename)\n",
    "                data.append({'video_path': filepath, 'label': label})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Usage\n",
    "dataset_dir = 'ESPD_trimmed_videos'\n",
    "output_csv = 'dataset_labels.csv'\n",
    "create_csv(dataset_dir, output_csv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6faded3f4fce0fe7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10ab1ec4f70e956"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset_labels.csv')\n",
    "\n",
    "# Count the number of videos in each class\n",
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_counts.index, class_counts.values, tick_label=['Arm Flapping', 'Headbanging', 'Spinning', 'Hand Action'])\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title('Number of Videos per Class')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff100c624565686"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pie Chart"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "621129b9c18b585f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pie chart for class distribution\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(class_counts, labels=['Arm Flapping', 'Headbanging', 'Spinning', 'Hand Action'], autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ff0334'])\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4542cd3123f5b372"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frames Per Video Distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a6d9c6789b495e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Get the number of frames per video\n",
    "df['num_frames'] = df['video_path'].apply(lambda x: int(cv2.VideoCapture(x).get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "# Create histogram for number of frames per video\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['num_frames'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of Frames')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title('Distribution of Number of Frames per Video')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "254a8e7fb3d7783c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average Frames per Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164ee1ce4e331ac3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate average number of frames per class\n",
    "avg_frames_per_class = df.groupby('label')['num_frames'].mean()\n",
    "\n",
    "# Bar plot for average frames per class\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(avg_frames_per_class.index, avg_frames_per_class.values, tick_label=['Arm Flapping', 'Headbanging', 'Spinning','Hand Action'], color=['#ff9999','#66b3ff','#99ff99','#ff0334'])\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Average Number of Frames')\n",
    "plt.title('Average Number of Frames per Class')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd54abb443bbfdfc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video Duration Distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3e02e1e4fe03979"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming a frame rate of 30 FPS\n",
    "frame_rate = 30\n",
    "df['duration'] = df['num_frames'] / frame_rate\n",
    "\n",
    "# Create histogram for video durations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['duration'], bins=20, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Video Duration (seconds)')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title('Distribution of Video Durations')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c382c684fe0ad4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Video to Frames Conversion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f76ddb42cb1941"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def video_to_frames(video_path, num_frames=20, resize=(300, 300)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            # If frame reading fails, append the last successful frame\n",
    "            if frames:\n",
    "                frames.append(frames[-1])\n",
    "            else:\n",
    "                # If no frames have been read yet, append a black image\n",
    "                frames.append(np.zeros((resize[1], resize[0], 3), dtype=np.uint8))\n",
    "    cap.release()\n",
    "    return frames\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e2b478701306b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('dataset_labels.csv')\n",
    "df['frames'] = df['video_path'].apply(lambda x: video_to_frames(x))\n",
    "df.to_pickle('dataset_frames.pkl')  # Save for later use"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bf346e4205ac768"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Frames For Each Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e38e562595945b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def show_sample_frames(df, num_frames=5):\n",
    "    # Get the unique labels (classes)\n",
    "    unique_labels = df['label'].unique()\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Get a sample video from the class\n",
    "        sample_video_path = df[df['label'] == label]['video_path'].values[0]\n",
    "        print(f\"Showing frames for class: {label}\")\n",
    "        \n",
    "        # Convert video to frames\n",
    "        frames = video_to_frames(sample_video_path, num_frames=num_frames)\n",
    "        \n",
    "        # Display sample frames\n",
    "        fig, axes = plt.subplots(1, num_frames, figsize=(15, 5))\n",
    "        for i, frame in enumerate(frames):\n",
    "            # Convert frame to RGB (if needed) for displaying with matplotlib\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(frame_rgb)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Show sample frames for each class\n",
    "show_sample_frames(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb199c872148c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Videos Grid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74adc757104fab1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_sample_grid(df, num_samples=3):\n",
    "    classes = df['label'].unique()\n",
    "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(15, 5 * len(classes)))\n",
    "\n",
    "    for i, class_label in enumerate(classes):\n",
    "        sample_videos = df[df['label'] == class_label].sample(num_samples)\n",
    "        for j, video_path in enumerate(sample_videos['video_path']):\n",
    "            frames = video_to_frames(video_path, num_frames=1)  # Extract just 1 frame\n",
    "            frame_rgb = cv2.cvtColor(frames[0], cv2.COLOR_BGR2RGB)\n",
    "            axes[i, j].imshow(frame_rgb)\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].set_title(f'Class: {class_label} - Video: {j+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show sample grid of frames\n",
    "show_sample_grid(df, num_samples=5)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd83a70d31753af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Child Detection and Cropping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a4464a7e0d7c9bc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def setup_detectron2():\n",
    "    cfg = get_cfg()\n",
    "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "    cfg.merge_from_file(\"D:/dataset/detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def crop_child(predictor, frame):\n",
    "    outputs = predictor(frame)\n",
    "    instances = outputs[\"instances\"]\n",
    "    if len(instances) == 0:\n",
    "        return frame  # If no detections, return original frame\n",
    "    # Assuming the first detected person is the target child\n",
    "    boxes = instances.pred_boxes.tensor.cpu().numpy()\n",
    "    scores = instances.scores.cpu().numpy()\n",
    "    # Select the box with the highest score\n",
    "    best_idx = np.argmax(scores)\n",
    "    box = boxes[best_idx]\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    # Ensure the coordinates are within frame boundaries\n",
    "    x1, y1 = max(x1, 0), max(y1, 0)\n",
    "    x2, y2 = min(x2, frame.shape[1]), min(y2, frame.shape[0])\n",
    "    cropped = frame[y1:y2, x1:x2]\n",
    "    print(\"2.Child cropped successfully\")\n",
    "    return cropped\n",
    "\n",
    "# Apply cropping to all frames\n",
    "def crop_all_frames(df, predictor):\n",
    "    cropped_frames = []\n",
    "    for frames in tqdm(df['frames'], desc=\"Cropping frames\"):\n",
    "        cropped_video = [crop_child(predictor, frame) for frame in frames]\n",
    "        cropped_frames.append(cropped_video)\n",
    "    df['cropped_frames'] = cropped_frames\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-22T21:50:10.009441100Z",
     "start_time": "2024-10-22T21:50:07.927905300Z"
    }
   },
   "id": "9bfb621d40a019bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create directory to save cropped images\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9d0fd075f0d270"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_dir_structure(base_dir):\n",
    "    dirs = ['train', 'test', 'val']\n",
    "    for d in dirs:\n",
    "        os.makedirs(os.path.join(base_dir, d), exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad4b1a55cd3914e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save cropped frames into respective folders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6440176c22ce539"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_cropped_frames(df, predictor, base_dir='data/image_split/'):\n",
    "    create_dir_structure(base_dir)\n",
    "    cropped_frames = []\n",
    "    for idx, frames in tqdm(enumerate(df['frames']), desc=\"Cropping and saving frames\"):\n",
    "        cropped_video = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            cropped_frame = crop_child(predictor, frame)\n",
    "            # Save the cropped frame\n",
    "            img_name = f\"img_{idx}_frame_{i}.jpg\"\n",
    "            folder = os.path.join(base_dir, 'all')  # Temporary folder for all data before splitting\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            cv2.imwrite(img_path, cropped_frame)\n",
    "            cropped_video.append(cropped_frame)\n",
    "        cropped_frames.append(cropped_video)\n",
    "    df['cropped_frames'] = cropped_frames\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "262cb0ca6c8757dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data into train, test, and validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2167218087c70c1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# Split data into train, test, and validation\n",
    "def split_data(base_dir='data/image_split/', test_size=0.2, val_size=0.1):\n",
    "    images = os.listdir(os.path.join(base_dir, 'all'))\n",
    "    train_val_images, test_images = train_test_split(images, test_size=test_size, random_state=42)\n",
    "    train_images, val_images = train_test_split(train_val_images, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    def move_images(image_list, split_type):\n",
    "        dest_dir = os.path.join(base_dir, split_type)\n",
    "        for image in image_list:\n",
    "            src = os.path.join(base_dir, 'all', image)\n",
    "            dst = os.path.join(dest_dir, image)\n",
    "            os.rename(src, dst)\n",
    "\n",
    "    move_images(train_images, 'train')\n",
    "    move_images(test_images, 'test')\n",
    "    move_images(val_images, 'val')\n",
    "\n",
    "    print(f\"Data split: {len(train_images)} train, {len(test_images)} test, {len(val_images)} val\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4339c442cd59a27f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24533dab73e9f063"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Predictor setup started\")\n",
    "predictor = setup_detectron2()\n",
    "print(\"Predictor setup successfully passed\")\n",
    "\n",
    "df = pd.read_pickle('dataset_frames.pkl')\n",
    "print(\"Cropping started\")\n",
    "df = save_cropped_frames(df, predictor)\n",
    "print(\"Cropping and saving passed\")\n",
    "\n",
    "# Save dataframe with cropped frames\n",
    "df.to_pickle('dataset_cropped_frames2.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7097267c276b7b56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_data()\n",
    "print(\"Data split into train, test, and val successfully\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8163a2413429989c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison of Cropped vs. Original Frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cc936da332b1b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_pickle('dataset_cropped_frames2.pkl')\n",
    "\n",
    "def video_to_frames(video_path, num_frames=20, resize=(300, 300)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            if frames:\n",
    "                frames.append(frames[-1])\n",
    "            else:\n",
    "                frames.append(np.zeros((resize[1], resize[0], 3), dtype=np.uint8))\n",
    "    cap.release()\n",
    "    return frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "328827258e2a814c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_cropped_vs_original(df, sample_idx=0, num_frames=5):\n",
    "    original_frames = video_to_frames(df['video_path'].iloc[sample_idx], num_frames=num_frames)\n",
    "    cropped_frames = df['cropped_frames'].iloc[sample_idx][:num_frames]\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_frames, figsize=(15, 6))\n",
    "    for i in range(num_frames):\n",
    "        # Original frame\n",
    "        frame_rgb = cv2.cvtColor(original_frames[i], cv2.COLOR_BGR2RGB)\n",
    "        axes[0, i].imshow(frame_rgb)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f'Original Frame {i+1}')\n",
    "        \n",
    "        # Cropped frame\n",
    "        if cropped_frames:\n",
    "            cropped_rgb = cv2.cvtColor(cropped_frames[i], cv2.COLOR_BGR2RGB)\n",
    "            axes[1, i].imshow(cropped_rgb)\n",
    "            axes[1, i].axis('off')\n",
    "            axes[1, i].set_title(f'Cropped Frame {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_cropped_vs_original(df, sample_idx=0, num_frames=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7420bbf941ebbc4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Feature Extraction with EfficientNet-B3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6559a2b5238325e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# frame_dataset.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),  # Resize all frames to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  \n",
    "                std=[0.229, 0.224, 0.225]    \n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.df.iloc[idx]['cropped_frames']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        frames = [self.transform(frame) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, label"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3141a9086b96b7b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(effnet, dataloader, device):\n",
    "    effnet.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_frames, batch_labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            batch_frames = batch_frames.to(device)\n",
    "            batch_size, num_frames, C, H, W = batch_frames.size()\n",
    "            batch_frames = batch_frames.view(-1, C, H, W)\n",
    "            features = effnet(batch_frames)\n",
    "            feature_dim = features.size(1)\n",
    "            features = features.view(batch_size, num_frames, feature_dim)\n",
    "            all_features.append(features.cpu())\n",
    "            all_labels.append(batch_labels.cpu())\n",
    "    \n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels\n",
    "\n",
    "# EfficientNet-B3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "effnet_b3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes= 500)\n",
    "effnet_b3._fc = torch.nn.Identity()\n",
    "effnet_b3.to(device)\n",
    "\n",
    "\n",
    "df = pd.read_pickle('dataset_cropped_frames2.pkl')\n",
    "dataset = FrameDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "features, labels = extract_features(effnet_b3, dataloader, device)\n",
    "\n",
    "# Save features and labels\n",
    "torch.save({'features': features, 'labels': labels}, 'extracted_features2.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f4bb1781f9724a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Preparing Data for Temporal Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439857ca464bdf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features  \n",
    "        self.labels = labels      \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "data = torch.load('extracted_features2.pth') \n",
    "\n",
    "features = data['features']  \n",
    "labels = data['labels']      \n",
    "\n",
    "dataset = ActionDataset(features, labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecdd9dcc8a9b7c09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple of (feature, label)\n",
    "        return self.features[idx], self.labels[idx]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45c8ceaada3f68db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the MS-TCN Model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5317becb38d62c27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a single-stage TCN block\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers, kernel_size=5):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        layers = []\n",
    "        dilation = 1\n",
    "        for _ in range(num_layers):\n",
    "            padding = (kernel_size - 1) * dilation // 2  # Calculate appropriate padding\n",
    "            layers.append(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "            )\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "            dilation *= 2  # Double the dilation at each layer\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.tcn(x)\n",
    "\n",
    "# Define the Multi-Stage TCN model\n",
    "class MSTCN(nn.Module):\n",
    "    def __init__(self, num_stages=3, in_channels=20, out_channels=256, num_classes=4):\n",
    "        super(MSTCN, self).__init__()\n",
    "        # Adjust to handle the input channels mismatch (1x1 conv to match input channels)\n",
    "        self.conv1x1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)  # in_channels=20\n",
    "        \n",
    "        # Create multiple stages of TCN blocks\n",
    "        self.stages = nn.ModuleList([TCNBlock(out_channels, out_channels, num_layers=5) for _ in range(num_stages)])\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(out_channels, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features, num_frames)\n",
    "        out = self.conv1x1(x)  # Shape: (batch_size, out_channels, num_frames)\n",
    "        for stage in self.stages:\n",
    "            out = stage(out) + out  # Residual connection\n",
    "        out = out.mean(dim=-1)  # Global average pooling over time (temporal dimension)\n",
    "        out = nn.ReLU()(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Dataset class to handle the features and labels\n",
    "class ActionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Load extracted features (adjust this to load your dataset appropriately)\n",
    "data = torch.load('extracted_features.pth')\n",
    "features = data['features']  # Shape: (num_samples, num_frames, feature_dim)\n",
    "labels = data['labels']  # Shape: (num_samples,)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ActionDataset(features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "print(features.shape)  # Check the shape of features before passing to model\n",
    "model = MSTCN(num_stages=3, in_channels=features.size(1), out_channels=256, num_classes=4).to(device)  # Match in_channels to features.size(1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Optimizer and loss function initialized\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b6b17fb1ea75c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloader, optimizer, criterion, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        print(f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for features, labels in dataloader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            # print(f\"Input shape: {features.shape}\")\n",
    "            outputs = model(features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels for F1-score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "        # Calculate F1-score for the epoch\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}, F1-Score: {f1:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, optimizer, criterion, num_epochs=50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "981ea99c27846518"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Defining the MS-TCN++ Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3189f92af364670"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Dual Dilated Conv1d block\n",
    "class DualDilatedConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, dilation=1):\n",
    "        super(DualDilatedConv1d, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=(kernel_size//2)*dilation, dilation=dilation)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=(kernel_size//2)*dilation, dilation=dilation)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        return out\n",
    "\n",
    "# Define Single Stage MS-TCN++ block\n",
    "class SingleStageMS_TCNpp(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=10):\n",
    "        super(SingleStageMS_TCNpp, self).__init__()\n",
    "        layers = []\n",
    "        dilation = 1\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(DualDilatedConv1d(hidden_channels, hidden_channels, dilation=dilation))\n",
    "            dilation *= 2  # Exponential dilation\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.conv1x1 = nn.Conv1d(hidden_channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, features, time)\n",
    "        out = self.network(x)\n",
    "        out = self.conv1x1(out)\n",
    "        return out\n",
    "\n",
    "class MSTCNPlusPlus(nn.Module):\n",
    "    def __init__(self, num_stages=4, num_layers=10, hidden_channels=64, num_classes=4, in_channels=20):\n",
    "        super(MSTCNPlusPlus, self).__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Initial 1x1 conv to match input channels to hidden channels\n",
    "        self.conv1x1_in = nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        \n",
    "        for _ in range(num_stages):\n",
    "            self.stages.append(SingleStageMS_TCNpp(hidden_channels, hidden_channels, hidden_channels, num_layers))\n",
    "\n",
    "        # Final 1x1 conv to map to num_classes\n",
    "        self.conv1x1_out = nn.Conv1d(hidden_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch, features, frames)\n",
    "        x = self.conv1x1_in(x)  # Match input channels to hidden_channels\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "\n",
    "        x = self.conv1x1_out(x)  # Match final hidden channels to num_classes\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = 4\n",
    "in_channels = features.size(2)  # Adjust based on your input\n",
    "hidden_channels = 64  # You can adjust this\n",
    "model = MSTCNPlusPlus(num_stages=4, num_layers=10, hidden_channels=hidden_channels, num_classes=num_classes, in_channels=in_channels)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c7c16f6c068c501"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Training the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12b82555602f5592"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load all features and labels (assuming they are PyTorch tensors)\n",
    "# features: shape (num_samples, num_frames, feature_dim)\n",
    "# labels: shape (num_samples,)\n",
    "features = features\n",
    "labels = labels\n",
    "\n",
    "# Define 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_classes = 4  # Assuming 4 output classes\n",
    "\n",
    "# To store results\n",
    "f1_scores = []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(features)):\n",
    "    print(f\"\\nStarting Fold {fold + 1}\")\n",
    "    \n",
    "    # Create subsets for training and validation\n",
    "    train_features = features[train_idx]\n",
    "    train_labels = labels[train_idx]\n",
    "    val_features = features[val_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    \n",
    "    # Create Dataloaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Instantiate a new model for each fold\n",
    "    in_channels = features.size(2)  # Number of input features (feature_dim)\n",
    "    model = MSTCNPlusPlus(num_stages=4, num_layers=10, hidden_channels=64, num_classes=num_classes, in_channels=in_channels)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()  # Modify weights if needed for imbalanced classes\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features = batch_features.to(device)  # (batch_size, num_frames, feature_dim)\n",
    "            batch_labels = batch_labels.to(device)      # (batch_size,)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)             # (batch_size, num_classes, num_frames)\n",
    "            outputs = outputs.mean(dim=2)               # Average over time (batch_size, num_classes)\n",
    "            loss = criterion(outputs, batch_labels)     # Compute loss\n",
    "            loss.backward()                             # Backpropagation\n",
    "            optimizer.step()                            # Update weights\n",
    "            running_loss += loss.item() * batch_features.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_labels in val_loader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_features)           # (batch_size, num_classes, num_frames)\n",
    "                outputs = outputs.mean(dim=2)             # Average over time (batch_size, num_classes)\n",
    "                _, preds = torch.max(outputs, 1)          # Get predictions\n",
    "                all_preds.extend(preds.cpu().numpy())     # Store predictions\n",
    "                all_targets.extend(batch_labels.cpu().numpy())  # Store true labels\n",
    "        \n",
    "        # Calculate weighted F1-score for the validation set\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Weighted F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # After training, evaluate on validation set\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Fold {fold + 1} Weighted F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Final results\n",
    "print(\"\\nCross-Validation Weighted F1-Scores:\", f1_scores)\n",
    "print(\"Average Weighted F1-Score:\", np.mean(f1_scores))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c88afad00f9091"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c23438b6bf3e1d99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\nFinal Evaluation:\")\n",
    "for fold, score in enumerate(f1_scores):\n",
    "    print(f\"Fold {fold + 1}: Weighted F1-Score = {score:.4f}\")\n",
    "print(f\"Average Weighted F1-Score across all folds: {np.mean(f1_scores):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b04d780b7d679480"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Average Weighted F1-Score across all folds: 0.7802"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfe900a7c51c925c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "befc3b378049a590"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
